{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./dataset/final/train.csv', usecols=['source', 'label'])\n",
    "train_df['source'] = train_df['source'].str.lower()\n",
    "train_df['label'] = train_df['label'].str.lower()\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "\n",
    "test_df = pd.read_csv('./dataset/final/test.csv', usecols=['source', 'label'])\n",
    "test_df['source'] = test_df['source'].str.lower()\n",
    "test_df['label'] = test_df['label'].str.lower()\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "dev_df = pd.read_csv('./dataset/final/dev.csv', usecols=['source', 'label'])\n",
    "dev_df['source'] = dev_df['source'].str.lower()\n",
    "dev_df['label'] = dev_df['label'].str.lower()\n",
    "dev_ds = Dataset.from_pandas(dev_df)\n",
    "\n",
    "all_df = pd.concat((train_df, test_df, dev_df))\n",
    "all_ds = Dataset.from_pandas(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./results_seq2seq/', local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('./results_seq2seq/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 . 9 . company_id - int , rank - int , company - str , headquarters - str , main_industry - str , sales_billion - float , profits_billion - float , assets_billion - float , market_value - float | how many companies that are not headquartered in the united states for each main industry ? show me a bar chart , and could you display by the total number from high to low ?\n",
      "mark bar encoding x main_industry y aggregate count main_industry transform filter headquarters != 'usa' sort y desc\n",
      "mark bar encoding x main_industry y aggregate count main_industry transform filter main_industry != 'united states' sort y desc\n",
      "\n",
      "19 . 8 . id - int , name - str , headquarters - str , industry - str , sales_billion - float , profits_billion - float , assets_billion - float , market_value_billion - float | , and order by the y axis in ascending .\n",
      "mark bar encoding x industry y aggregate count industry transform sort y asc\n",
      "mark bar encoding x headquarters y aggregate none market_value_billion transform sort y asc\n",
      "\n",
      "3 . 8 . cust_id - int , cust_name - str , acc_type - str , acc_bal - int , no_of_loans - int , credit_score - int , branch_id - int , state - str | visualize a bar chart for what are the names and account balances of customers with the letter 'a' in their names ? , could you sort from high to low by the y axis ?\n",
      "mark bar encoding x cust_name y aggregate none acc_bal transform filter cust_name like '%a%' sort y desc\n",
      "mark bar encoding x acc_type y aggregate none acc_bal transform filter credit_score = 'a' sort y desc\n",
      "\n",
      "11 . 6 . dept_code - str , dept_name - str , school_code - str , emp_num - int , dept_address - str , dept_extension - int | return a bar chart on how many departments are in each school ? , and list by the count(distinctdept_name) in ascending .\n",
      "mark bar encoding x school_code y aggregate count distinct dept_name transform sort count(distinct asc dept_name) asc\n",
      "mark bar encoding x school_code y aggregate count distinct dept_name transform sort count(distinctdept_name) asc\n",
      "\n",
      "50 . 4 . id - int , name - str , dept_name - str , salary - float | give me a histogram for what are the names and average salaries for departments with average salary higher than 42000 ? , and could you order by the y-axis from high to low ?\n",
      "mark bar encoding x dept_name y aggregate mean salary transform sort y desc\n",
      "mark bar encoding x name y aggregate mean salary transform filter salary > 42000 sort y desc\n",
      "\n",
      "58 . 8 . facid - int , lname - str , fname - str , rank - str , sex - str , phone - int , room - str , building - str | how many faculty members do we have for each rank and gender ? plot them as bar chart , and list from low to high by the names .\n",
      "mark bar encoding x rank y aggregate count rank color sex transform sort x asc\n",
      "mark bar encoding x rank y aggregate count rank transform sort x asc\n",
      "\n",
      "16 . 8 . season - float , player - str , position - str , country - int , team - int , draft_pick_number - int , draft_class - str , college - str | show the draft pick numbers and draft classes of players whose positions are defenders in a bar chart , and list by the draft_class from high to low .\n",
      "mark bar encoding x draft_class y aggregate none draft_pick_number transform filter position = \"defender\" sort x desc\n",
      "mark bar encoding x position y aggregate count position transform filter position = \"usa\" sort x desc\n",
      "\n",
      "15 . 5 . guest_id - int , gender_code - str , guest_first_name - str , guest_last_name - str , date_of_birth - str | what are the number of dates of birth of all the guests whose gender is \"male\" ?\n",
      "mark bar encoding x date_of_birth y aggregate count date_of_birth transform filter gender_code = \"male\" bin x by weekday\n",
      "mark bar encoding x sex_code y aggregate count sex_code transform filter sex_last_name = \"male\" bin x by weekday\n",
      "\n",
      "34 . 8 . stuid - int , lname - str , fname - str , age - int , sex - str , major - int , advisor - int , city_code - str | visualize a bar chart for what are the average ages for male and female students ? , display x axis in asc order please .\n",
      "mark bar encoding x sex y aggregate mean age transform sort x asc\n",
      "mark bar encoding x sex y aggregate mean age transform filter sex = 'm' sort x asc\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b5b9c76ff91d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'▁'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             )\n\u001b[1;32m   1252\u001b[0m             \u001b[0;31m# 12. run beam search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m             return self.beam_search(\n\u001b[0m\u001b[1;32m   1254\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   2079\u001b[0m             \u001b[0mnext_token_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m             next_token_scores, next_tokens = torch.topk(\n\u001b[0m\u001b[1;32m   2082\u001b[0m                 \u001b[0mnext_token_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m             )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from VisAwareTranslation import postprocessing\n",
    "\n",
    "nl_template_cnt = 0\n",
    "nl_template_match = 0\n",
    "\n",
    "for source, label in zip(test_df['source'], test_df['label']):\n",
    "    input_ids = tokenizer(source, return_tensors=\"pt\", max_length=512, padding=True, truncation=True).input_ids \n",
    "    outputs = model.generate(input_ids)\n",
    "    \n",
    "    decoded = ''.join(tokenizer.convert_ids_to_tokens(outputs[0])[1:-1]).replace('▁', ' ').strip()\n",
    "    \n",
    "    pred = postprocessing(label, decoded)\n",
    "\n",
    "    nl_template_cnt += 1\n",
    "\n",
    "    if ' '.join(label.replace('\"', \"'\").split()) == ' '.join(pred.replace('\"', \"'\").split()):\n",
    "        nl_template_match += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = test_df['source'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(source, return_tensors=\"pt\", max_length=512, padding=True, truncation=True).input_ids \n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "decoded = ''.join(tokenizer.convert_ids_to_tokens(outputs[0])[1:-1]).replace('▁', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11 . 7 . id - int , train_number - int , name - str , origin - str , destination - str , time - str , interval - str | find the number of trains starting from each origin plot them as bar chart , and order in asc by the y-axis .'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mark bar encoding x origin y aggregate count origin transform sort y asc'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./results_causal/', local_files_only=True)\n",
    "model = AutoModelForCausalLM.from_pretrained('./results_causal/', local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = '10 . 9 . company_id - int , rank - int , company - str , headquarters - str , main_industry - str , sales_billion - float , profits_billion - float , assets_billion - float , market_value - float | what is the market value of every'\n",
    "\n",
    "input_ids = tokenizer(source, return_tensors=\"pt\").input_ids \n",
    "\n",
    "logits = model(input_ids).logits[:, -1, :]\n",
    "\n",
    "pred_ids = torch.argsort(logits)[0, -5:]\n",
    "pred_words = [tokenizer.decode(pred_id) for pred_id in pred_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' major', ' industry', ' asset', ' manufacturer', ' company']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-113.4453, -113.0419, -111.8864, -111.2938, -106.9644],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids = torch.argsort(logits)[0, -5:]\n",
    "probs = logits[0][pred_ids] \n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-113.4453, -113.0419, -111.8864, -111.2938, -106.9644],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0][pred_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20380668, 0.203082  , 0.20100617, 0.19994149, 0.19216363],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1688,  2831, 11171, 11554,  1664])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
